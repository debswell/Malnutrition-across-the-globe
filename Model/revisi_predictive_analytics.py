# -*- coding: utf-8 -*-
"""Revisi_Predictive Analytics.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bhrZC8vB634b1pe4kf5OSdhd9BX1H4EC

## Nama : Debi Welani Christin Saragih
## ID : MC009D5X0246
## Kelas : MC-35

# Data Loading
"""

# Commented out IPython magic to ensure Python compatibility.
#import libabry yang dibutuhkan
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
# %matplotlib inline
import seaborn as sns

"""**Penjelasan**

- **NumPy (`np`)**: Untuk operasi numerik dan manipulasi array.
- **Matplotlib (`plt`)**: Untuk membuat berbagai jenis grafik visualisasi data.
- **Pandas (`pd`)**: Untuk memproses, membaca, dan menganalisis data tabular seperti DataFrame.
- **Seaborn (`sns`)**: Untuk membuat visualisasi statistik yang lebih kompleks dan estetik.
- **`%matplotlib inline`**: Agar semua plot dari Matplotlib muncul langsung di dalam notebook Jupyter.

"""

#load dataset
df = pd.read_csv('https://raw.githubusercontent.com/debswell/Malnutrition-across-the-globe/refs/heads/main/Dataset/malnutrition-estimates.csv')
df

"""**Penjelasan**

* pd.read_csv(): Membaca dataset berformat CSV dari URL.
*URL: Mengambil data malnutrisi langsung dari GitHub.
*df: Menyimpan data ke dalam DataFrame untuk analisis lebih lanjut.
*df (tanpa fungsi): Menampilkan isi DataFrame di output notebook.

**Kesimpulan :** Dataset yang dipakai terdiri dari 924 record dan 20 kolom

# EDA (Exploratory Data Analysis)
"""

#Mencek informasi pada dataset
df.info()

"""**Penjelasan**
* df.info() digunakan untuk menampilkan informasi ringkas tentang DataFrame.

* Informasi yang ditampilkan meliputi:

  - Tipe kolom 8 object dan 12 Numeric
  - Terdapat beberapa kolom null.
"""

#Mencek deskripsi statistik data
df.describe()

"""**Penjelasan**

* df.describe() digunakan untuk menampilkan statistik deskriptif dari kolom numerik dalam DataFrame.
* Informasi yang ditampilkan meliputi:

  - Count (jumlah data), Mean (rata-rata), Std (standar deviasi),
  - Min, 25%, 50% (median), 75%, dan Max.


"""

#Menghapus beberapa fitur yang tidak dibutuhkan
df.drop(['Unnamed: 0','ISO code','Survey Year','Survey Sample (N)','Notes','Report Author','Source','Short Source','LLDC or SID2'],axis=1,inplace=True)
df.info()

"""**Penjelasan**
- Digunakan fungsi `df.drop()` untuk menghapus kolom-kolom yang dianggap tidak relevan dengan analisis.
- Parameter `axis=1` menunjukkan bahwa kolom yang dihapus.
- Parameter `inplace=True` membuat perubahan langsung pada DataFrame tanpa perlu disimpan ke variabel baru.
- Setelah penghapusan, `df.info()` digunakan kembali untuk melihat struktur DataFrame terbaru.


**Kesimpulan :**

=> Dari 20 kolom yang terdapat pada dataset, tersisi 13 kolom atau fitur yang akan digunakan dalam pembangunan model. 1 fitur dengan tipe object dan lainnya bertipe Numeric
"""

#Cek Missing Values
df.isnull().sum()

"""**Penjelasan :**
- Menggunakan fungsi `df.isnull().sum()` untuk memeriksa jumlah nilai yang hilang (missing values) pada setiap kolom di DataFrame.
- Fungsi `isnull()` akan menghasilkan DataFrame boolean (True jika nilai kosong).
- Fungsi `sum()` akan menghitung jumlah nilai kosong untuk masing-masing kolom.


**Kesimpulan :**
=> Pada dataset terdapat beberapa missing value,  dan paling banyak terdapat pada severe wasting sebanyak 228. Aapabila missing value tersebut dihapus sepertinya kurang efektif karena jumlah dataset yang dimiliki hanya 924 baris. Apabila dilakukan penghapusan pada dataset yang kosong, nanti terdapat banyak informasi yang hilang. Oleh karena itu untuk mengatasi missing values kita dengan cara sederhana akan menggunakan nilai mean pada fitur dengan tipe number.
"""

#Mengatasi missing value dengan nilai mean
df_numeric = df.select_dtypes(include=['float64', 'int64'])
df = df.fillna(df_numeric.mean())
df.isnull().sum()

"""**Penjelasan**

- Memilih kolom bertipe numerik (float64 dan int64) menggunakan df.select_dtypes().
- Mengisi nilai yang hilang (missing values) pada dataset dengan rata-rata (mean) dari masing-masing kolom numerik menggunakan df.fillna().
- Dengan cara ini, data menjadi lengkap dan siap untuk analisis lebih lanjut tanpa adanya nilai kosong.
- Setelah pengisian, dicek kembali jumlah missing value dengan df.isnull().sum() untuk memastikan semua nilai kosong sudah terisi.

"""

#Cek duplikat data pada semua kolom
df.duplicated().sum()

"""**Penjelasan**
untuk melihat apakah ada data yang duplikat atau tidak, dapat mnggunakan perintah duplicated. untuk menghitung jumlah totalnya menggunakan sum().
Pada dataet df tidak ada data duplicate
"""

#Cek outlier pada fitur
n = len(df_numeric)
baris = (n // 3) + 1  # atur jumlah baris untuk subplot jika banyak fitur
plt.figure(figsize=(15, 5 * baris))
for i, col in enumerate(df_numeric, 1):
    plt.subplot(baris, 4, i)  # atur jadi 3 kolom per baris
    sns.boxplot(y=df[col])
    plt.title(f'Outlier {col}')
plt.tight_layout()  # <- ini harus pakai tanda kurung
plt.show()

"""**Penjelasan**
* Cek Outlier pada Fitur

- Menggunakan boxplot untuk mendeteksi keberadaan outlier pada setiap fitur numerik dalam dataset.
- Jumlah subplot disesuaikan dengan banyaknya fitur, diatur menjadi 3 kolom per baris agar tampilan rapi.
- sns.boxplot() dari library Seaborn digunakan untuk membuat visualisasi boxplot tiap fitur.
- Boxplot membantu mengidentifikasi data yang berada di luar rentang normal (outlier).
- plt.tight_layout() memastikan subplot tidak saling tumpang tindih agar grafik terlihat jelas.
- Visualisasi ini penting untuk memahami distribusi data dan mendeteksi nilai ekstrim yang mungkin perlu penanganan khusus.


**Kesimpulan :**

Ada beberapa fitur yang memiliki outlier seperti pada kolom Serve wasting, wasting,overweight,underweight dan population
"""

#Hapus outliers
Q1 = df_numeric.quantile(0.25)
Q3 = df_numeric.quantile(0.75)
IQR = Q3 - Q1
filter_outliers = ~((df_numeric < (Q1 - 1.5 * IQR)) | (df_numeric > (Q3 + 1.5 * IQR))).any(axis=1)
df = df[filter_outliers]
df.shape

"""**Penjelasan**
* Menghapus Outlier

- Menghitung nilai kuartil pertama (Q1) dan kuartil ketiga (Q3) dari data numerik.
- Menghitung IQR (Interquartile Range) yaitu selisih antara Q3 dan Q1.
- Menentukan data yang bukan outlier dengan aturan:  
  nilai tidak boleh kurang dari (Q1 - 1.5 * IQR) dan tidak boleh lebih dari (Q3 + 1.5 * IQR).
- Baris data yang mengandung outlier akan dihapus dari DataFrame.
- Setelah penghapusan, df.shape menunjukkan ukuran dataset yang baru ( 752 dan 11).

"""

#Ekploratory fitur kategorikal
count = df['Country'].value_counts()
percent = 100*df['Country'].value_counts(normalize=True)
df_categorical= pd.DataFrame({'Jumlah : ':count,'Persentase':percent.round(1)})
df_categorical

"""**Penjelasan**
* Eksplorasi Fitur Kategorikal

- Menghitung jumlah kemunculan setiap kategori pada kolom **`Country`** menggunakan value_counts().
- Menghitung persentase masing-masing kategori terhadap total data dengan value_counts(normalize=True) lalu dikalikan 100.
- Membuat DataFrame baru df_categorical yang berisi jumlah dan persentase tiap negara.
- Data ini membantu mengetahui distribusi data berdasarkan negara secara ringkas dan jelas.

"""

#Explore fitur numeric
df.hist(bins=50, figsize=(20,15))
plt.show()

"""**Penjelasan**
* Eksplorasi Fitur Numerik

- Membuat histogram untuk setiap fitur numerik dalam dataset menggunakan df.hist().
- Histogram membantu melihat sebaran data, frekuensi nilai, dan pola distribusi.
- bins=50 artinya data dibagi menjadi 50 interval untuk visualisasi yang lebih detail.
- Ukuran gambar diatur besar agar plot terlihat jelas (figsize=(20,15)).
- Visualisasi ini berguna untuk memahami karakteristik tiap fitur numerik secara cepat.

"""

# Mengamati hubungan antar fitur numerik dengan fungsi pairplot()
sns.pairplot(df, diag_kind = 'kde')

"""**Penjelasan**
* Mengamati Hubungan Antar Fitur Numerik
- Menggunakan sns.pairplot() dari Seaborn untuk membuat grafik scatterplot antar semua pasangan fitur numerik.
- diag_kind='kde' menampilkan grafik distribusi (density plot) pada diagonal.
- Visualisasi ini membantu melihat pola hubungan, korelasi, dan distribusi data antar fitur secara sekaligus.
- Sangat berguna untuk eksplorasi awal sebelum analisis lanjutan.


=> Dilihat pada hubungan fitur, ada 3 fitur yang hubungannya tidak jelas dengan stunting karena plotnya hanya berbentuk vertikal lurus saja tetapi untuk melihat lebih jelasnya dapat digunakan metode heatmap dan hubungannya yang sedikit acak dengan stunting yaitu Populasi
"""

# Visualisasi distribusi target (Stunning)
plt.figure(figsize=(10, 6))
sns.histplot(df['Stunting'], kde=True)
plt.title('Distribusi Stunting')
plt.show()

"""**Penjelasan**
* Visualisasi Distribusi Target (Stunting)
- Membuat histogram dari kolom Stunting untuk melihat sebaran nilai target.
- sns.histplot() digunakan dengan opsi kde=True agar ditampilkan juga kurva kepadatan (density curve).
- Ukuran gambar diatur agar grafik mudah dilihat (figsize=(10, 6)).


**Kesimpulan :**

Pada grafik sebelumnya, kurang jelas bagaimana tingkat stunting. Pada grafik ini terlihat jelas bahwa stunting tertinggi terjadi pada rentang usia 30 bulan (sekitar 2 tahun) dengan total kasus adalah sebanyak 100 lebih. Kasus stunting menurun terjadi mulai usia 50 bulan
"""

#cek dengan menggunakan heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(df_numeric.corr(), annot=True, cmap='coolwarm')
plt.show()

"""**Penjelasan**
* Cek Korelasi dengan Heatmap
- Menggunakan heatmap untuk melihat hubungan (korelasi) antar fitur numerik dalam dataset.
- Fungsi df_numeric.corr() menghitung nilai korelasi antar kolom numerik.
- Parameter annot=True menampilkan angka korelasi di setiap kotak heatmap.
- cmap='coolwarm' memberikan gradasi warna yang menunjukkan kekuatan dan arah korelasi (positif atau negatif).
- Visualisasi ini membantu mengetahui fitur mana yang saling berhubungan kuat atau lemah.


**Kesimpulan :**

Fitur yang ada cukup memiliki korelasi yang baik terhadap fitur target(stunting). Fitur yang memiliki korelasi paling lemah dengan stunting adalah populasi yaitu hanya 0.14, artinya populasi tidak terlalu berpengaruh terhadap kasus stunting.

# Data Preparation
"""

# Encoding pada fitur 'Country'
country_mean = df.groupby('Country')['Stunting'].mean().to_dict()
df['Country_encoded'] = df['Country'].map(country_mean)

"""**Penjelasan**
* Encoding pada Fitur 'Country'
- Menghitung rata-rata nilai Stunting untuk setiap negara menggunakan groupby() dan mean().
- Membuat dictionary dari hasil rata-rata tersebut untuk memetakan setiap negara ke nilai rata-rata Stunting-nya.
- Mengubah kolom Country menjadi kolom numerik baru Country_encoded dengan mengganti nama negara menjadi nilai rata-rata Stunting dari negara tersebut.
- Teknik ini membantu mengubah data kategori menjadi angka yang bisa digunakan dalam analisis atau model machine learning.

"""

# Frequency Encoding pada fitur 'Country'
country_freq = df['Country'].value_counts().to_dict()
df['Country_freq'] = df['Country'].map(country_freq)

"""**Penjelasan**
* Frequency Encoding pada Fitur 'Country'
- Menghitung frekuensi kemunculan setiap negara di kolom Country menggunakan value_counts().
- Membuat dictionary dari frekuensi tersebut untuk memetakan setiap negara ke jumlah kemunculannya.
- Menambahkan kolom baru Country_freq yang berisi nilai frekuensi sebagai representasi numerik dari negara.
- Teknik ini membantu mengubah data kategori menjadi angka berdasarkan seberapa sering kategori tersebut muncul dalam data.

"""

# Reduksi Dimensi dengan PCA untuk fitur Wasting
from sklearn.decomposition import PCA
pca = PCA(n_components=1, random_state=123)
pca.fit(df[['Severe Wasting', 'Wasting']])
df['dim_Wasting'] = pca.transform(df.loc[:, ('Severe Wasting', 'Wasting')]).flatten()
# Tampilkan variance explained
print("\nVariance explained by PCA:", pca.explained_variance_ratio_.round(3))

"""**Penjelasan**
* Reduksi Dimensi dengan PCA pada Fitur Wasting
- Menggunakan PCA (Principal Component Analysis) untuk menggabungkan dua fitur numerik yaitu Severe Wasting dan Wasting menjadi satu fitur baru.
- n_components=1 berarti kita mereduksi dua fitur menjadi satu komponen utama.
- PCA mencari kombinasi fitur yang menangkap sebagian besar variasi data.
- Hasil transformasi disimpan di kolom baru dim_Wasting.
- pca.explained_variance_ratio_ menunjukkan proporsi variasi data yang berhasil dijelaskan oleh komponen utama tersebut.
- Teknik ini membantu menyederhanakan data tanpa kehilangan informasi penting.  

"""

# Drop kolom asli setelah PCA
df.drop(['Severe Wasting', 'Wasting'], axis=1, inplace=True)

"""**Penjelasan**
* Menghapus Kolom Asli Setelah PCA
- Menghapus kolom asli Severe Wasting dan Wasting dari DataFrame setelah digantikan oleh kolom baru hasil PCA.
- Hal ini dilakukan agar data menjadi lebih ringkas dan menghindari duplikasi informasi.
- Parameter axis=1 menandakan penghapusan kolom, dan inplace=True membuat perubahan langsung pada DataFrame.

"""

# Tambahkan feature interaksi
df['Underweight_Overweight'] = df['Underweight'] * df['Overweight']
df['Pop_Income'] = df["U5 Population ('000s)"] * df['Income Classification']

"""**Penjelasan**
* Menambahkan Feature Interaksi
- Membuat fitur baru Underweight_Overweight dengan mengalikan nilai pada kolom Underweight dan Overweight.  
  Tujuannya untuk menangkap hubungan interaksi antara dua fitur tersebut.
- Membuat fitur baru Pop_Income dengan mengalikan U5 Population ('000s) dan Income Classification.  
  Fitur ini bisa menggambarkan efek gabungan dari populasi balita dan tingkat pendapatan suatu negara.
- Feature interaksi ini dapat membantu model memahami hubungan kompleks antar fitur yang sederhana.

"""

# Tambahkan feature polynomial untuk fitur penting
df['Year_squared'] = df['Year'] ** 2

"""**Penjelasan**
* Menambahkan Feature Polinomial
- Membuat fitur baru Year_squared dengan mengkuadratkan nilai pada kolom Year.
- Fitur polinomial ini dapat membantu model menangkap hubungan non-linear yang mungkin ada antara tahun dan target.
- Dengan menambahkan fitur ini, model bisa belajar pola yang lebih kompleks dari data.

"""

# Persiapan data untuk modeling
X = df.drop(["Stunting", "Country"], axis=1)  # Drop Country asli, gunakan encoded
X['Country'] = df['Country_encoded']  # Gunakan target encoding
y = df["Stunting"]

"""**Penjelasan**
* Persiapan Data untuk Modeling
- Membuat fitur input X dengan menghapus kolom target Stunting dan kolom kategori asli Country dari DataFrame.
- Menambahkan kembali kolom Country tapi menggunakan hasil encoding (kolom Country_encoded) supaya data kategori menjadi numerik.
- Membuat variabel target y yang berisi nilai dari kolom Stunting.
- Data ini siap digunakan untuk proses pelatihan model machine learning.

"""

#Train Test Split
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=123)
#Cek jumlah sampel
print(f'Jumlah data: {len(df)}')
print(f'Jumlah sampel X_train: {X_train.shape[0]}, Jumlah sampel X_test: {X_test.shape[0]}')

"""**Penjelasan**
* Membagi Data Menjadi Train dan Test Set serta Mengecek Jumlah Sampel
- Data dibagi menjadi dua bagian:  
  - **Training set** (80%) untuk melatih model.  
  - **Testing set** (20%) untuk menguji model.  
- random_state=123 memastikan pembagian data konsisten setiap kali dijalankan.  
- Setelah pembagian, dilakukan pengecekan jumlah data keseluruhan dan jumlah sampel di masing-masing set (train dan test).  
- Ini penting untuk memastikan data sudah terbagi dengan benar sesuai proporsi yang diinginkan.

"""

#Standarisasi data
from sklearn.preprocessing import StandardScaler

numerical_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()
scaler = StandardScaler()
scaler.fit(X_train[numerical_features])
X_train_scaled = X_train.copy()
X_test_scaled = X_test.copy()
X_train_scaled[numerical_features] = scaler.transform(X_train[numerical_features])
X_test_scaled[numerical_features] = scaler.transform(X_test[numerical_features])

"""**Penjelasan**
* Standarisasi Data Numerik
- Menggunakan StandardScaler untuk menstandarisasi fitur numerik agar memiliki mean 0 dan standar deviasi 1.
- Standarisasi membantu model machine learning agar fitur dengan skala berbeda dapat diproses secara seimbang.
- Langkahnya:
  - Pilih fitur numerik dari data.
  - fit() scaler pada data training untuk menghitung rata-rata dan standar deviasi.
  - Terapkan transformasi transform() ke data training dan testing.
- Data hasil standarisasi disimpan dalam variabel baru X_train_scaled dan X_test_scaled.

"""

# Cek statistik data setelah scaling
print("\nStatistik data setelah scaling:")
print(X_train_scaled[numerical_features].describe().round(4))

"""**Penjelasan**
* Mengecek Statistik Data Setelah Scaling
- Menampilkan ringkasan statistik (count, mean, std, min, max, dll) untuk fitur numerik setelah proses standarisasi.
- Tujuannya untuk memastikan bahwa fitur-fitur tersebut sudah memiliki nilai rata-rata (mean) 0 dan standar deviasi (std) 1.
- Ini membantu memastikan proses scaling berjalan dengan benar.

# Modelling
"""

metrics = ['MAE', 'RMSE', 'R2']
models_train = pd.DataFrame(index=metrics, columns=['LinearRegression', 'Ridge', 'Lasso', 'RandomForest', 'XGBoost'])
models_test = pd.DataFrame(index=metrics, columns=['LinearRegression', 'Ridge', 'Lasso', 'RandomForest', 'XGBoost'])

"""**Penjelasan**
* Menyiapkan DataFrame untuk Menyimpan Hasil Evaluasi Model
- Membuat dua DataFrame kosong, yaitu models_train dan models_test, untuk menyimpan metrik evaluasi dari berbagai model pada data training dan testing.
- Metrik yang digunakan adalah:  
  - **MAE (Mean Absolute Error)**  
  - **RMSE (Root Mean Squared Error)**  
  - **R2 (R-squared)**  
- Kolom DataFrame berisi nama-nama model yang akan dibandingkan:  
  - Linear Regression  
  - Ridge Regression  
  - Lasso Regression  
  - Random Forest  
  - XGBoost  
- Ini memudahkan dalam membandingkan performa berbagai model secara terstruktur.

"""

#Import Library
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# 1. Linear Regression sebagai baseline
lr = LinearRegression()
lr.fit(X_train_scaled, y_train)
y_train_pred_lr = lr.predict(X_train_scaled)
y_test_pred_lr = lr.predict(X_test_scaled)

models_train.loc['MAE', 'LinearRegression'] = mean_absolute_error(y_train, y_train_pred_lr)
models_train.loc['RMSE', 'LinearRegression'] = np.sqrt(mean_squared_error(y_train, y_train_pred_lr))
models_train.loc['R2', 'LinearRegression'] = r2_score(y_train, y_train_pred_lr)

models_test.loc['MAE', 'LinearRegression'] = mean_absolute_error(y_test, y_test_pred_lr)
models_test.loc['RMSE', 'LinearRegression'] = np.sqrt(mean_squared_error(y_test, y_test_pred_lr))
models_test.loc['R2', 'LinearRegression'] = r2_score(y_test, y_test_pred_lr)

"""**Penjelasan**
* Melatih dan Mengevaluasi Model Linear Regression
- Import beberapa library penting dari scikit-learn untuk modeling dan evaluasi.  
- Membuat model Linear Regression sebagai baseline (model dasar untuk perbandingan).  
- Melatih model dengan data training yang sudah distandarisasi (X_train_scaled, y_train).  
- Melakukan prediksi untuk data training dan testing.  
- Menyimpan hasil metrik evaluasi ke dalam DataFrame models_train dan models_test untuk kemudian dibandingkan dengan model lain.

"""

# 2. Ridge Regression (untuk mengatasi multicollinearity)
from sklearn.linear_model import Ridge
ridge = Ridge(alpha=1.0, random_state=123)
ridge.fit(X_train_scaled, y_train)
y_train_pred_ridge = ridge.predict(X_train_scaled)
y_test_pred_ridge = ridge.predict(X_test_scaled)

models_train.loc['MAE', 'Ridge'] = mean_absolute_error(y_train, y_train_pred_ridge)
models_train.loc['RMSE', 'Ridge'] = np.sqrt(mean_squared_error(y_train, y_train_pred_ridge))
models_train.loc['R2', 'Ridge'] = r2_score(y_train, y_train_pred_ridge)

models_test.loc['MAE', 'Ridge'] = mean_absolute_error(y_test, y_test_pred_ridge)
models_test.loc['RMSE', 'Ridge'] = np.sqrt(mean_squared_error(y_test, y_test_pred_ridge))
models_test.loc['R2', 'Ridge'] = r2_score(y_test, y_test_pred_ridge)

"""**Penjelasan**
* Melatih dan Mengevaluasi Model Ridge Regression
- Model **Ridge Regression** digunakan untuk mengatasi masalah multikolinearitas (fitur yang saling berkorelasi tinggi).  
- Parameter alpha=1.0 mengontrol kekuatan regularisasi.  
- Melatih model dengan data training yang sudah distandarisasi.  
- Melakukan prediksi pada data training dan testing.  
- Menghitung metrik evaluasi: MAE, RMSE, dan R2.  
- Hasil evaluasi disimpan dalam DataFrame models_train dan models_test untuk perbandingan dengan model lain.  

"""

# 3. Lasso Regression (untuk feature selection)
from sklearn.linear_model import Lasso
lasso = Lasso(alpha=0.1, random_state=123)
lasso.fit(X_train_scaled, y_train)
y_train_pred_lasso = lasso.predict(X_train_scaled)
y_test_pred_lasso = lasso.predict(X_test_scaled)

models_train.loc['MAE', 'Lasso'] = mean_absolute_error(y_train, y_train_pred_lasso)
models_train.loc['RMSE', 'Lasso'] = np.sqrt(mean_squared_error(y_train, y_train_pred_lasso))
models_train.loc['R2', 'Lasso'] = r2_score(y_train, y_train_pred_lasso)

models_test.loc['MAE', 'Lasso'] = mean_absolute_error(y_test, y_test_pred_lasso)
models_test.loc['RMSE', 'Lasso'] = np.sqrt(mean_squared_error(y_test, y_test_pred_lasso))
models_test.loc['R2', 'Lasso'] = r2_score(y_test, y_test_pred_lasso)

"""**Penjelasan**
* Melatih dan Mengevaluasi Model Lasso Regression
- **Lasso Regression** digunakan untuk melakukan feature selection dengan menekan koefisien fitur yang kurang penting menjadi nol.  
- Parameter alpha=0.1 mengontrol kekuatan regularisasi.  
- Melatih model dengan data training yang sudah distandarisasi.  
- Melakukan prediksi pada data training dan testing.  
- Menghitung metrik evaluasi: MAE, RMSE, dan R2.  
- Hasil evaluasi disimpan dalam DataFrame models_train dan models_test untuk dibandingkan dengan model lainnya.

"""

# 4. Random Forest dengan parameter yang mengurangi overfitting
rf = RandomForestRegressor(
    n_estimators=100,    # Lebih banyak trees
    max_depth=5,         # Batasi kedalaman untuk mengurangi overfitting
    min_samples_split=5, # Hindari split pada sampel yang terlalu kecil
    min_samples_leaf=2,  # Hindari leaf dengan sampel terlalu sedikit
    max_features='sqrt', # Batasi jumlah fitur yang dipertimbangkan per split
    random_state=123,
    n_jobs=-1
)
rf.fit(X_train_scaled, y_train)
y_train_pred_rf = rf.predict(X_train_scaled)
y_test_pred_rf = rf.predict(X_test_scaled)

models_train.loc['MAE', 'RandomForest'] = mean_absolute_error(y_train, y_train_pred_rf)
models_train.loc['RMSE', 'RandomForest'] = np.sqrt(mean_squared_error(y_train, y_train_pred_rf))
models_train.loc['R2', 'RandomForest'] = r2_score(y_train, y_train_pred_rf)

models_test.loc['MAE', 'RandomForest'] = mean_absolute_error(y_test, y_test_pred_rf)
models_test.loc['RMSE', 'RandomForest'] = np.sqrt(mean_squared_error(y_test, y_test_pred_rf))
models_test.loc['R2', 'RandomForest'] = r2_score(y_test, y_test_pred_rf)

"""**Penjelasn**
* Melatih dan Mengevaluasi Model Random Forest

- **Random Forest** adalah metode ensemble yang menggabungkan banyak decision tree untuk meningkatkan akurasi dan mengurangi overfitting.  
- Parameter yang digunakan bertujuan mengurangi overfitting, antara lain:  
  - n_estimators=100: jumlah pohon dalam hutan.  
  - max_depth=5: membatasi kedalaman pohon agar tidak terlalu kompleks.  
  - min_samples_split=5 dan min_samples_leaf=2: mencegah pohon tumbuh dari data yang terlalu sedikit.  
  - max_features='sqrt': jumlah fitur yang dipertimbangkan di tiap split dibatasi.  
- Melatih model dengan data training yang sudah distandarisasi.  
- Melakukan prediksi pada data training dan testing.  
- Menghitung metrik evaluasi: MAE, RMSE, dan R2.  
- Hasil evaluasi disimpan dalam DataFrame models_train dan models_test untuk perbandingan dengan model lain.  

"""

# 5. XGBoost dengan regularisasi untuk mengurangi overfitting
import xgboost as xgb
xgb_reg = xgb.XGBRegressor(
    n_estimators=100,
    learning_rate=0.05,  # Lower learning rate
    max_depth=3,         # Shallow trees to prevent overfitting
    subsample=0.8,       # Use 80% of data per tree
    colsample_bytree=0.8, # Use 80% of features per tree
    reg_alpha=0.1,       # L1 regularization
    reg_lambda=1.0,      # L2 regularization
    random_state=123
)
xgb_reg.fit(X_train_scaled, y_train)
y_train_pred_xgb = xgb_reg.predict(X_train_scaled)
y_test_pred_xgb = xgb_reg.predict(X_test_scaled)

models_train.loc['MAE', 'XGBoost'] = mean_absolute_error(y_train, y_train_pred_xgb)
models_train.loc['RMSE', 'XGBoost'] = np.sqrt(mean_squared_error(y_train, y_train_pred_xgb))
models_train.loc['R2', 'XGBoost'] = r2_score(y_train, y_train_pred_xgb)

models_test.loc['MAE', 'XGBoost'] = mean_absolute_error(y_test, y_test_pred_xgb)
models_test.loc['RMSE', 'XGBoost'] = np.sqrt(mean_squared_error(y_test, y_test_pred_xgb))
models_test.loc['R2', 'XGBoost'] = r2_score(y_test, y_test_pred_xgb)

"""**Penjelasan**
* Melatih dan Mengevaluasi Model XGBoost
- **XGBoost** adalah algoritma boosting yang kuat untuk regresi dan klasifikasi.  
- Parameter yang digunakan untuk mengurangi overfitting:  
  - n_estimators=100: jumlah pohon (trees) yang dibuat.  
  - learning_rate=0.05: laju pembelajaran rendah agar model belajar lebih pelan dan stabil.  
  - max_depth=3: membatasi kedalaman pohon agar tidak terlalu kompleks.  
  - subsample=0.8: hanya menggunakan 80% data untuk tiap pohon, meningkatkan generalisasi.  
  - colsample_bytree=0.8: hanya menggunakan 80% fitur untuk tiap pohon.  
  - reg_alpha=0.1 dan reg_lambda=1.0: regularisasi L1 dan L2 untuk menghindari overfitting.  
- Melatih model dengan data training yang sudah distandarisasi.  
- Melakukan prediksi pada data training dan testing.  
- Menghitung metrik evaluasi: MAE, RMSE, dan R2.  
- Menyimpan hasil evaluasi di DataFrame models_train dan models_test untuk dibandingkan dengan model lain.  

"""

# Tampilkan hasil evaluasi
print("\nTrain metrics:")
print(models_train)
print("\nTest metrics:")
print(models_test)

"""**Penjelasan**
* Untuk mengevaluasi performa model regresi dalam memprediksi tingkat stunting, digunakan tiga metrik utama:

* MAE (Mean Absolute Error): Rata-rata selisih absolut antara nilai aktual dan prediksi. Semakin kecil nilainya, semakin baik model.

* RMSE (Root Mean Squared Error): Akar dari rata-rata kuadrat selisih prediksi dan aktual. Lebih sensitif terhadap kesalahan besar.

* R² (R-squared / Koefisien Determinasi): Mengukur seberapa baik variabel independen menjelaskan variabel dependen. Nilai mendekati 1 menunjukkan model yang baik.

**Kesimpulan :**

Pada hasil evaluasi, model dengan hasil yang baik adalah Ridge dan Lasso tetapi apabila dibandingkan ahsil metrik antara Ridge dan Lasso, model yang paling baik adalah Ridge dengan perbandingan mterik pada data train dan data test yang lebih kecil. Oleh karena itu kita akan menggunakan model regresi Ridge dalam pembangunan model untuk melakukan prediksi
"""

# Visualisasi perbandingan model
plt.figure(figsize=(15, 10))

plt.subplot(2, 2, 1)
models_test.loc['MAE'].plot(kind='bar', color='skyblue')
plt.title('MAE (Lower is Better)')
plt.grid(axis='y', linestyle='--', alpha=0.7)

plt.subplot(2, 2, 2)
models_test.loc['RMSE'].plot(kind='bar', color='salmon')
plt.title('RMSE (Lower is Better)')
plt.grid(axis='y', linestyle='--', alpha=0.7)

plt.subplot(2, 2, 3)
models_test.loc['R2'].plot(kind='bar', color='lightgreen')
plt.title('R² Score (Higher is Better)')
plt.grid(axis='y', linestyle='--', alpha=0.7)

plt.subplot(2, 2, 4)
plt.scatter(y_test, y_test_pred_rf, alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
plt.xlabel('Actual')
plt.ylabel('Predicted')
plt.title('Best Model: Actual vs Predicted')
plt.grid(True, linestyle='--', alpha=0.7)

plt.tight_layout()
plt.show()

"""Visualisasi Hasil Evaluasi:

- **MAE (Lower is Better)**: Memberikan gambaran rata-rata error absolut dari setiap model.
- **RMSE (Lower is Better)**: Menunjukkan seberapa besar kesalahan prediksi dalam skala kuadrat.
- **R² Score (Higher is Better)**: Memperlihatkan seberapa baik model menjelaskan variasi data target.
- **Scatter Plot Actual vs Predicted**: Digunakan untuk membandingkan nilai aktual dan hasil prediksi dari model (menggunakan Random Forest sebagai ilustrasi). Semakin dekat titik-titik ke garis diagonal, semakin baik prediksi model.

Kesimpulan Sementara:
- **Ridge Regression** menunjukkan performa yang paling stabil antara data latih dan uji, dengan R² tertinggi di data uji dan MAE/RMSE yang relatif kecil.
- **XGBoost** dan **Random Forest** sangat akurat di data latih, tetapi menunjukkan kemungkinan overfitting karena performanya menurun di data uji.
- **Linear Regression** dan **Lasso Regression** cukup kompetitif, dengan performa sedikit di bawah Ridge.
"""

# FINALISASI MODEL RIDGE REGRESSION

# Inisialisasi model Ridge dengan parameter optimal
# tuning alpha (opsional)
ridge_model = Ridge(alpha=1.0, random_state=123)

# Latih model menggunakan seluruh data training
print("Melatih model Ridge Regression...")
ridge_model.fit(X_train_scaled, y_train)

"""* Finalisasi Model: Ridge Regression

Model Ridge Regression dipilih untuk difinalisasi karena memberikan performa paling stabil dan seimbang antara data latih dan data uji. Ridge cocok untuk menangani multicollinearity dan mencegah overfitting dengan regularisasi L2. Pada tahap ini, model dilatih ulang menggunakan seluruh data training dengan parameter `alpha=1.0`, dan siap digunakan untuk prediksi lebih lanjut atau implementasi nyata.

"""

# Evaluasi model
y_train_pred = ridge_model.predict(X_train_scaled)
y_test_pred = ridge_model.predict(X_test_scaled)

"""* Evaluasi Model Ridge Regression

Setelah model Ridge Regression dilatih, kita lakukan evaluasi dengan menghasilkan prediksi pada data training dan testing.

Kode berikut memprediksi nilai target berdasarkan fitur yang sudah distandarisasi:

- y_train_pred berisi prediksi pada data training, untuk mengevaluasi performa model terhadap data yang sudah dipelajari.
- y_test_pred berisi prediksi pada data testing, untuk mengukur kemampuan model dalam memprediksi data baru.

Prediksi ini akan digunakan untuk menghitung metrik evaluasi seperti MAE, RMSE, dan R², sehingga kita bisa memahami seberapa baik model bekerja sebelum digunakan untuk aplikasi nyata.

"""

# Tampilkan hasil evaluasi
print("\nEvaluasi model Ridge Regression:")
print(f"Train R²: {r2_score(y_train, y_train_pred):.4f}")
print(f"Test R²: {r2_score(y_test, y_test_pred):.4f}")
print(f"Train MAE: {mean_absolute_error(y_train, y_train_pred):.4f}")
print(f"Test MAE: {mean_absolute_error(y_test, y_test_pred):.4f}")
print(f"Train RMSE: {np.sqrt(mean_squared_error(y_train, y_train_pred)):.4f}")
print(f"Test RMSE: {np.sqrt(mean_squared_error(y_test, y_test_pred)):.4f}")

"""* Menampilkan Hasil Evaluasi Model Ridge Regression

Kode ini menampilkan nilai metrik evaluasi model Ridge Regression baik pada data training maupun data testing:

- **R² Score**: Mengukur seberapa baik model menjelaskan variasi data. Nilai maksimal 1.0, semakin mendekati 1 berarti model semakin baik.
- **MAE (Mean Absolute Error)**: Rata-rata selisih absolut antara nilai aktual dan prediksi. Nilai lebih kecil lebih baik.
- **RMSE (Root Mean Squared Error)**: Mengukur rata-rata kuadrat selisih prediksi dan aktual, dengan penalti yang lebih besar untuk kesalahan besar. Nilai lebih kecil lebih baik.

Dengan melihat metrik ini, kita bisa menilai apakah model telah bekerja secara konsisten dan akurat di kedua set data.

"""

# VISUALISASI HASIL MODEL

# Plot prediksi vs aktual
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_test_pred, alpha=0.6)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
plt.xlabel('Nilai Aktual')
plt.ylabel('Nilai Prediksi')
plt.title('Ridge Regression: Nilai Aktual vs Prediksi')
plt.grid(True, linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

"""* Visualisasi Hasil Prediksi Model Ridge Regression

Kode ini membuat grafik scatter plot untuk membandingkan nilai aktual (y_test) dengan nilai prediksi (y_test_pred) dari model Ridge Regression pada data testing.

- Setiap titik pada plot merepresentasikan satu sampel data.
- Garis merah putus-putus (r--) menunjukkan garis ideal di mana nilai prediksi sama persis dengan nilai aktual.
- Semakin dekat titik-titik dengan garis ini, semakin baik model dalam melakukan prediksi.

Visualisasi ini membantu memahami sejauh mana model mampu memprediksi nilai target dengan akurat secara visual.


**Kesimpulan :**

sesuai dari plot prediksi dan aktual yang didapatkan, model yang dibangun cukup baik pada nilai aktual
"""

# Plot residual
plt.figure(figsize=(10, 6))
residuals = y_test - y_test_pred
plt.scatter(y_test_pred, residuals, alpha=0.6)
plt.axhline(y=0, color='r', linestyle='--')
plt.xlabel('Nilai Prediksi')
plt.ylabel('Residual')
plt.title('Ridge Regression: Plot Residual')
plt.grid(True, linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

"""* Plot Residual Model Ridge Regression

Plot residual ini menampilkan selisih antara nilai aktual (y_test) dengan nilai prediksi (y_test_pred), yaitu residual, terhadap nilai prediksi.

- Residual = Nilai Aktual - Nilai Prediksi.
- Garis horizontal merah putus-putus di nilai 0 menandakan residual nol (prediksi sempurna).
- Titik-titik yang tersebar secara acak dan seimbang di sekitar garis nol menunjukkan bahwa model tidak memiliki pola kesalahan sistematis dan asumsi linearitas terpenuhi.

Plot residual penting untuk memeriksa apakah model mengalami bias, heteroskedastisitas, atau masalah lain dalam prediksi.

"""

# Histogram residual
plt.figure(figsize=(10, 6))
plt.hist(residuals, bins=20, alpha=0.7, color='skyblue', edgecolor='black')
plt.axvline(x=0, color='r', linestyle='--')
plt.xlabel('Residual')
plt.ylabel('Frekuensi')
plt.title('Ridge Regression: Distribusi Residual')
plt.grid(True, linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

"""* Distribusi Residual Model Ridge Regression

Histogram ini menunjukkan distribusi residual (selisih antara nilai aktual dan nilai prediksi) dari model Ridge Regression.

- Residual yang tersebar simetris dan mendekati distribusi normal (terpusat di sekitar nol) menandakan model menghasilkan prediksi yang baik tanpa bias sistematis.
- Garis vertikal merah putus-putus di nol menunjukkan titik pusat distribusi residual.
- Analisis distribusi residual penting untuk memastikan asumsi normalitas error dalam model regresi, yang berpengaruh pada validitas inferensi statistik dan keakuratan model.

"""

# LEARNING CURVE UNTUK MEMASTIKAN TIDAK ADA OVERFITTING

# Plot learning curve
from sklearn.model_selection import learning_curve
from sklearn.model_selection import KFold
def plot_learning_curve(estimator, X, y, title, ylim=None, cv=None,
                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):
    plt.figure(figsize=(10, 6))
    plt.title(title)
    if ylim is not None:
        plt.ylim(*ylim)
    plt.xlabel("Jumlah sampel training")
    plt.ylabel("Score")

    train_sizes, train_scores, test_scores = learning_curve(
        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring='r2')

    train_scores_mean = np.mean(train_scores, axis=1)
    train_scores_std = np.std(train_scores, axis=1)
    test_scores_mean = np.mean(test_scores, axis=1)
    test_scores_std = np.std(test_scores, axis=1)

    plt.grid()
    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,
                     train_scores_mean + train_scores_std, alpha=0.1, color="r")
    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,
                     test_scores_mean + test_scores_std, alpha=0.1, color="g")
    plt.plot(train_sizes, train_scores_mean, 'o-', color="r", label="Training score")
    plt.plot(train_sizes, test_scores_mean, 'o-', color="g", label="Cross-validation score")
    plt.legend(loc="best")
    return plt

# Plot learning curve
cv = KFold(n_splits=5, shuffle=True, random_state=123)
plot_learning_curve(ridge_model, X_train_scaled, y_train,
                    "Learning Curve (Ridge Regression)", ylim=(0.7, 1.01), cv=cv)
plt.tight_layout()
plt.show()

"""* Learning Curve untuk Model Ridge Regression

Learning curve digunakan untuk memeriksa performa model pada data training dan validasi seiring dengan bertambahnya jumlah data training.

- Kurva training score (warna merah) menunjukkan seberapa baik model belajar dari data training.
- Kurva cross-validation score (warna hijau) menunjukkan seberapa baik model dapat melakukan generalisasi pada data yang belum pernah dilihat.
- Jika kedua kurva konvergen pada skor yang tinggi, berarti model tidak mengalami overfitting atau underfitting.
- Jarak besar antara kedua kurva menandakan kemungkinan overfitting (training score tinggi, validation score rendah).
- Kurva ini membantu memastikan bahwa model Ridge Regression yang digunakan memiliki performa stabil dan tidak overfit pada data training.


**Kesimpulan :**

Hasil akurasi yang didapatkan cukup baik itu terjadi pada akurasi lebih dari 90% pada jumlah sampel 400 an
"""

# MELAKUKAN PREDIKSI DENGAN SKENARIO UNTUK MELATIH MODEL

# 1. Pertama, pastikan kita memiliki daftar fitur yang tepat
print("=" * 60)
print("INFORMASI MODEL")
print("=" * 60)
print("Fitur-fitur yang digunakan dalam model:")
feature_list = X_train_scaled.columns.tolist()
for i, feature in enumerate(feature_list, 1):
    print(f"{i:2d}. {feature}")
print(f"\nTotal fitur: {len(feature_list)}")

# 2. Definisi fungsi predict_stunting yang diperbaiki
def predict_stunting(model, scaler, sample_data, required_features):
    """
    Melakukan prediksi stunting menggunakan model yang sudah dilatih.

    Parameters:
    -----------
    model : sklearn model
        Model yang sudah dilatih (misalnya Ridge, RandomForest, dll)
    scaler : sklearn scaler
        Scaler yang digunakan untuk normalisasi data
    sample_data : dict
        Dictionary berisi data input untuk prediksi
    required_features : list
        Daftar fitur yang diperlukan oleh model

    Returns:
    --------
    float : Prediksi prevalensi stunting
    """
    import pandas as pd
    import numpy as np

    # Buat DataFrame dari sample_data
    df_sample = pd.DataFrame([sample_data])

    # Pastikan urutan kolom sesuai dengan yang digunakan saat training
    df_sample = df_sample.reindex(columns=required_features, fill_value=0)

    # Lakukan scaling menggunakan scaler yang sama saat training
    sample_scaled = scaler.transform(df_sample)

    # Buat DataFrame untuk hasil scaling agar tetap memiliki nama kolom
    # Ini akan menghilangkan warning sklearn
    sample_scaled_df = pd.DataFrame(sample_scaled, columns=required_features)

    # Prediksi menggunakan DataFrame dengan nama kolom
    prediction = model.predict(sample_scaled_df)[0]

    # Pastikan prediksi dalam rentang yang masuk akal (0-100%)
    prediction = max(0, min(100, prediction))

    return prediction

# 3. Buat fungsi helper yang diperbaiki
def prepare_prediction_data(sample_data, required_features):
    """
    Memastikan data prediksi memiliki semua fitur yang diperlukan oleh model.

    Parameters:
    -----------
    sample_data : dict
        Dictionary berisi data input
    required_features : list
        Daftar fitur yang diperlukan oleh model

    Returns:
    --------
    dict : Dictionary yang sudah dilengkapi dengan semua fitur yang diperlukan
    """
    # Buat copy dari data asli
    complete_data = sample_data.copy()

    # Periksa fitur yang hilang
    missing_features = []
    for feature in required_features:
        if feature not in complete_data:
            missing_features.append(feature)
            # Set nilai default yang lebih masuk akal
            if feature == 'Country':
                complete_data[feature] = 103  # Contoh: encoding untuk Indonesia
            else:
                complete_data[feature] = 0

    if missing_features:
        print(f"Menambahkan {len(missing_features)} fitur yang hilang:")
        for feature in missing_features:
            value = complete_data[feature]
            print(f"{feature}: {value}")
        print()

    return complete_data

# 4. Skenario untuk Indonesia tahun 2030
print("\n" + "=" * 60)
print("SKENARIO PREDIKSI: INDONESIA 2030")
print("=" * 60)

try:
    # Data untuk Indonesia - lengkap dengan semua fitur
    indonesia_2030 = {
        'Year': 2030,
        'Income Classification': 1,  # Lower income
        'LDC': 0,  # Bukan Least Developed Country
        'LIFD': 0,  # Bukan Low Income Food Deficit
        'Overweight': 35.5,
        'Underweight': 60.0,
        "U5 Population ('000s)": 250000,
        'Country_encoded': 35.0,  # Target encoding untuk Indonesia
        'Country_freq': 15,  # Frequency encoding
        'dim_Wasting': 0.5,
        'Underweight_Overweight': 5.5 * 15.0,  # 82.5
        'Pop_Income': 25000 * 1,  # 25000
        'Year_squared': 2030 ** 2,
        'Country': 103  # Label encoding untuk Indonesia
    }

    print("Data Input untuk Prediksi:")
    print("-" * 40)
    for key, value in indonesia_2030.items():
        if isinstance(value, float):
            print(f"   {key:<25}: {value:.2f}")
        else:
            print(f"   {key:<25}: {value}")

    print("\n Memproses data untuk prediksi...")

    # Pastikan semua fitur yang diperlukan tersedia
    complete_data = prepare_prediction_data(indonesia_2030, X_train_scaled.columns)

    # Prediksi
    print("Melakukan prediksi...")
    prediction = predict_stunting(ridge_model, scaler, complete_data, X_train_scaled.columns)

    print("\n" + "=" * 60)
    print("HASIL PREDIKSI")
    print("=" * 60)
    print(f"🇮🇩 Negara: Indonesia")
    print(f"Tahun: 2030")
    print(f"Prediksi Prevalensi Stunting: {prediction:.2f}%")

    # Interpretasi hasil
    print("\n Interpretasi Hasil:")
    if prediction < 10:
        interpretation = "Sangat Rendah - Target WHO tercapai"
        emoji = "🟢"
    elif prediction < 20:
        interpretation = "Rendah - Mendekati target WHO"
        emoji = "🟡"
    elif prediction < 30:
        interpretation = "Sedang - Perlu upaya lebih"
        emoji = "🟠"
    else:
        interpretation = "Tinggi - Perlu intervensi serius"
        emoji = "🔴"

    print(f"   {emoji} Status: {interpretation}")
    print("=" * 60)

except Exception as e:
    print("\n ERROR DALAM PREDIKSI")
    print("=" * 60)
    print(f"Error: {e}")
    print("\n Pastikan variabel berikut sudah didefinisikan:")
    print("ridge_model: Model yang sudah dilatih")
    print("scaler: StandardScaler atau scaler lainnya")
    print("X_train_scaled: DataFrame dengan kolom fitur yang sudah di-scale")
    print("=" * 60)

"""## Prediksi Prevalensi Stunting dengan Skenario Tahun 2025 (Indonesia)

### 1. Menampilkan Informasi Fitur Model
Pertama-tama, kita mencetak daftar semua fitur yang digunakan dalam model regresi (Ridge Regression). Hal ini penting untuk memastikan input prediksi memiliki semua fitur yang diperlukan.

```python
feature_list = X_train_scaled.columns.tolist()

### 2. Fungsi predict_stunting()
Fungsi ini digunakan untuk melakukan prediksi prevalensi stunting berdasarkan data input skenario.

Parameter:

* model: Model regresi yang sudah dilatih.
* scaler: Objek StandardScaler atau scaler lain yang digunakan untuk normalisasi saat training.
* sample_data: Dictionary berisi input prediksi.
* required_features: Daftar nama fitur yang digunakan saat training.

Fungsi ini akan:

* Membuat DataFrame dari dictionary input.
* Menyusun kolom sesuai urutan fitur training.
* Melakukan scaling terhadap data input.
* Menghasilkan prediksi prevalensi stunting yang dibatasi antara 0–100%.

### 3. Fungsi prepare_prediction_data()
Fungsi ini digunakan untuk memastikan bahwa data prediksi memiliki semua fitur yang diperlukan oleh model.

Jika ada fitur yang belum tersedia dalam input:

* Fungsi akan menambahkan fitur tersebut dengan nilai default (misalnya 0, atau 103 untuk Indonesia).
* Menampilkan informasi fitur yang ditambahkan.
* Hal ini mencegah error akibat input yang tidak lengkap saat dilakukan prediksi.

### 4. Skenario Prediksi untuk Indonesia Tahun 2025
Contoh skenario prediksi dibuat untuk negara Indonesia pada tahun 2025 dengan asumsi nilai beberapa indikator seperti:

* Persentase underweight: 60%
* Overweight: 35.5%
* Populasi anak di bawah 5 tahun: 250.000
* Encoding negara Indonesia: 103
* Prediksi dilakukan dengan langkah:
* Menampilkan input skenario.
* Memastikan fitur lengkap melalui prepare_prediction_data().
* Melakukan prediksi melalui predict_stunting().

### 5. Interpretasi Hasil Prediksi
Setelah memperoleh hasil prediksi, model menginterpretasikan nilai tersebut menjadi kategori:

🔴 Tinggi (≥30%) → Perlu intervensi serius

🟠 Sedang (20–30%) → Perlu upaya lebih

🟡 Rendah (10–20%) → Mendekati target WHO

🟢 Sangat Rendah (<10%) → Target WHO tercapai

### 6. Penanganan Error
Jika terjadi kesalahan, sistem akan:

* Menampilkan pesan error.

* Menyarankan variabel penting yang perlu dipastikan sudah didefinisikan (ridge_model, scaler, dan X_train_scaled).
"""

